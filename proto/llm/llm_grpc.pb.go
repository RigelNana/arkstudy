// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
// versions:
// - protoc-gen-go-grpc v1.5.1
// - protoc             v3.19.6
// source: llm/llm.proto

package llm

import (
	context "context"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
)

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
// Requires gRPC-Go v1.64.0 or later.
const _ = grpc.SupportPackageIsVersion9

const (
	LLMService_AskQuestion_FullMethodName        = "/llm.LLMService/AskQuestion"
	LLMService_AskQuestionStream_FullMethodName  = "/llm.LLMService/AskQuestionStream"
	LLMService_SemanticSearch_FullMethodName     = "/llm.LLMService/SemanticSearch"
	LLMService_GenerateEmbeddings_FullMethodName = "/llm.LLMService/GenerateEmbeddings"
	LLMService_UpsertChunks_FullMethodName       = "/llm.LLMService/UpsertChunks"
)

// LLMServiceClient is the client API for LLMService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
type LLMServiceClient interface {
	// MVP 第一阶段：基础问答
	AskQuestion(ctx context.Context, in *QuestionRequest, opts ...grpc.CallOption) (*QuestionResponse, error)
	// 流式问答：逐 token/分片返回
	AskQuestionStream(ctx context.Context, in *QuestionRequest, opts ...grpc.CallOption) (grpc.ServerStreamingClient[TokenChunk], error)
	// MVP 第一阶段：语义检索
	SemanticSearch(ctx context.Context, in *SearchRequest, opts ...grpc.CallOption) (*SearchResponse, error)
	// MVP 第一阶段：生成向量嵌入
	GenerateEmbeddings(ctx context.Context, in *EmbeddingRequest, opts ...grpc.CallOption) (*EmbeddingResponse, error)
	// 批量分片入库（更高吞吐）
	UpsertChunks(ctx context.Context, in *UpsertChunksRequest, opts ...grpc.CallOption) (*UpsertChunksResponse, error)
}

type lLMServiceClient struct {
	cc grpc.ClientConnInterface
}

func NewLLMServiceClient(cc grpc.ClientConnInterface) LLMServiceClient {
	return &lLMServiceClient{cc}
}

func (c *lLMServiceClient) AskQuestion(ctx context.Context, in *QuestionRequest, opts ...grpc.CallOption) (*QuestionResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(QuestionResponse)
	err := c.cc.Invoke(ctx, LLMService_AskQuestion_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) AskQuestionStream(ctx context.Context, in *QuestionRequest, opts ...grpc.CallOption) (grpc.ServerStreamingClient[TokenChunk], error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	stream, err := c.cc.NewStream(ctx, &LLMService_ServiceDesc.Streams[0], LLMService_AskQuestionStream_FullMethodName, cOpts...)
	if err != nil {
		return nil, err
	}
	x := &grpc.GenericClientStream[QuestionRequest, TokenChunk]{ClientStream: stream}
	if err := x.ClientStream.SendMsg(in); err != nil {
		return nil, err
	}
	if err := x.ClientStream.CloseSend(); err != nil {
		return nil, err
	}
	return x, nil
}

// This type alias is provided for backwards compatibility with existing code that references the prior non-generic stream type by name.
type LLMService_AskQuestionStreamClient = grpc.ServerStreamingClient[TokenChunk]

func (c *lLMServiceClient) SemanticSearch(ctx context.Context, in *SearchRequest, opts ...grpc.CallOption) (*SearchResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(SearchResponse)
	err := c.cc.Invoke(ctx, LLMService_SemanticSearch_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) GenerateEmbeddings(ctx context.Context, in *EmbeddingRequest, opts ...grpc.CallOption) (*EmbeddingResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(EmbeddingResponse)
	err := c.cc.Invoke(ctx, LLMService_GenerateEmbeddings_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *lLMServiceClient) UpsertChunks(ctx context.Context, in *UpsertChunksRequest, opts ...grpc.CallOption) (*UpsertChunksResponse, error) {
	cOpts := append([]grpc.CallOption{grpc.StaticMethod()}, opts...)
	out := new(UpsertChunksResponse)
	err := c.cc.Invoke(ctx, LLMService_UpsertChunks_FullMethodName, in, out, cOpts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// LLMServiceServer is the server API for LLMService service.
// All implementations must embed UnimplementedLLMServiceServer
// for forward compatibility.
type LLMServiceServer interface {
	// MVP 第一阶段：基础问答
	AskQuestion(context.Context, *QuestionRequest) (*QuestionResponse, error)
	// 流式问答：逐 token/分片返回
	AskQuestionStream(*QuestionRequest, grpc.ServerStreamingServer[TokenChunk]) error
	// MVP 第一阶段：语义检索
	SemanticSearch(context.Context, *SearchRequest) (*SearchResponse, error)
	// MVP 第一阶段：生成向量嵌入
	GenerateEmbeddings(context.Context, *EmbeddingRequest) (*EmbeddingResponse, error)
	// 批量分片入库（更高吞吐）
	UpsertChunks(context.Context, *UpsertChunksRequest) (*UpsertChunksResponse, error)
	mustEmbedUnimplementedLLMServiceServer()
}

// UnimplementedLLMServiceServer must be embedded to have
// forward compatible implementations.
//
// NOTE: this should be embedded by value instead of pointer to avoid a nil
// pointer dereference when methods are called.
type UnimplementedLLMServiceServer struct{}

func (UnimplementedLLMServiceServer) AskQuestion(context.Context, *QuestionRequest) (*QuestionResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method AskQuestion not implemented")
}
func (UnimplementedLLMServiceServer) AskQuestionStream(*QuestionRequest, grpc.ServerStreamingServer[TokenChunk]) error {
	return status.Errorf(codes.Unimplemented, "method AskQuestionStream not implemented")
}
func (UnimplementedLLMServiceServer) SemanticSearch(context.Context, *SearchRequest) (*SearchResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method SemanticSearch not implemented")
}
func (UnimplementedLLMServiceServer) GenerateEmbeddings(context.Context, *EmbeddingRequest) (*EmbeddingResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method GenerateEmbeddings not implemented")
}
func (UnimplementedLLMServiceServer) UpsertChunks(context.Context, *UpsertChunksRequest) (*UpsertChunksResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method UpsertChunks not implemented")
}
func (UnimplementedLLMServiceServer) mustEmbedUnimplementedLLMServiceServer() {}
func (UnimplementedLLMServiceServer) testEmbeddedByValue()                    {}

// UnsafeLLMServiceServer may be embedded to opt out of forward compatibility for this service.
// Use of this interface is not recommended, as added methods to LLMServiceServer will
// result in compilation errors.
type UnsafeLLMServiceServer interface {
	mustEmbedUnimplementedLLMServiceServer()
}

func RegisterLLMServiceServer(s grpc.ServiceRegistrar, srv LLMServiceServer) {
	// If the following call pancis, it indicates UnimplementedLLMServiceServer was
	// embedded by pointer and is nil.  This will cause panics if an
	// unimplemented method is ever invoked, so we test this at initialization
	// time to prevent it from happening at runtime later due to I/O.
	if t, ok := srv.(interface{ testEmbeddedByValue() }); ok {
		t.testEmbeddedByValue()
	}
	s.RegisterService(&LLMService_ServiceDesc, srv)
}

func _LLMService_AskQuestion_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(QuestionRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).AskQuestion(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: LLMService_AskQuestion_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).AskQuestion(ctx, req.(*QuestionRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_AskQuestionStream_Handler(srv interface{}, stream grpc.ServerStream) error {
	m := new(QuestionRequest)
	if err := stream.RecvMsg(m); err != nil {
		return err
	}
	return srv.(LLMServiceServer).AskQuestionStream(m, &grpc.GenericServerStream[QuestionRequest, TokenChunk]{ServerStream: stream})
}

// This type alias is provided for backwards compatibility with existing code that references the prior non-generic stream type by name.
type LLMService_AskQuestionStreamServer = grpc.ServerStreamingServer[TokenChunk]

func _LLMService_SemanticSearch_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(SearchRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).SemanticSearch(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: LLMService_SemanticSearch_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).SemanticSearch(ctx, req.(*SearchRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_GenerateEmbeddings_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(EmbeddingRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).GenerateEmbeddings(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: LLMService_GenerateEmbeddings_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).GenerateEmbeddings(ctx, req.(*EmbeddingRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _LLMService_UpsertChunks_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(UpsertChunksRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(LLMServiceServer).UpsertChunks(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: LLMService_UpsertChunks_FullMethodName,
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(LLMServiceServer).UpsertChunks(ctx, req.(*UpsertChunksRequest))
	}
	return interceptor(ctx, in, info, handler)
}

// LLMService_ServiceDesc is the grpc.ServiceDesc for LLMService service.
// It's only intended for direct use with grpc.RegisterService,
// and not to be introspected or modified (even as a copy)
var LLMService_ServiceDesc = grpc.ServiceDesc{
	ServiceName: "llm.LLMService",
	HandlerType: (*LLMServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "AskQuestion",
			Handler:    _LLMService_AskQuestion_Handler,
		},
		{
			MethodName: "SemanticSearch",
			Handler:    _LLMService_SemanticSearch_Handler,
		},
		{
			MethodName: "GenerateEmbeddings",
			Handler:    _LLMService_GenerateEmbeddings_Handler,
		},
		{
			MethodName: "UpsertChunks",
			Handler:    _LLMService_UpsertChunks_Handler,
		},
	},
	Streams: []grpc.StreamDesc{
		{
			StreamName:    "AskQuestionStream",
			Handler:       _LLMService_AskQuestionStream_Handler,
			ServerStreams: true,
		},
	},
	Metadata: "llm/llm.proto",
}
